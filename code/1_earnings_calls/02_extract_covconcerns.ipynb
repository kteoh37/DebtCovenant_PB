{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c84e28",
   "metadata": {},
   "source": [
    "Extracts covenant concerns from earnings call transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "ps = PorterStemmer()\n",
    "bucket = '[bucket_name]'  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate expect keywords\n",
    "\n",
    "expect_keywords_all = [\n",
    "    'may becom','hope','outlook','go to','tailwind','work toward','trend','is like to','may depend','may not','forse','would','seek to','ought','potenti','could depend','unknown','remain confid','shortterm','depend','endeavor','intend','abl to remain','feel','may result','project','expect to','possibl','like will result','goal','may affect','go forward','belief','consid','estim will','contempl','suggest','pursu','call for','appear','well posit to','think','with a view to','appear to','up to','short term','prioriti','hypothes','can have','indic','may impact','schedul','envis','believ','could','look forward','pro forma','drive','uncertain','explor','could be','look forward to','see','prospect','upsid','may','should','is like','risk','improv','longterm','like','uncertainti','tent','forese','predict','would be','headwind','view','move toward','aim','estim','on target','pend','probabl','could potenti','might','may be','are like','pipelin','do not expect','may continu','seek','will','shall','not expect','will like result','futur','unanticip','guidanc','look ahead','likelihood','like to','full year guidanc','anticip','confid','opportun','propos','on pace','plan','schedul to','preliminari','will like','will like be','do not anticip','expect','presum','express confid','can be','opportunity','plans','believes','could potentially','is likely to','drive','predicting','may affect','may continue','uncertain','expect','headwind','would be','shall','depend','expressed confidence','projects','aims','looking forward','scheduled to','think','hopefully','on target','presume','seek to','view','looks forward','expects','belief','pending','may not','suggests','moving toward','depends','believe','goals','trend','do not expect','appear to'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic data cleaning. return tokens\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "\n",
    "    # remove formatting\n",
    "    sentence = re.sub('\\n',' ', sentence) # remove line break markers \n",
    "    sentence = re.sub('&#[0-9]+;',' ', sentence) # remove character ids\n",
    "    \n",
    "    # remove months etc\n",
    "    sentence = re.sub('covenant skills','', sentence)\n",
    "    sentence = re.sub('customer covenant','', sentence)\n",
    "    sentence = re.sub(r\"\\b(?:'ll|we'll|will|may|should|shouldn't|can|can't|would|wouldn't|can also|may also|will also|should also) \\b(?:increase|decrease|step down|step up|see|say|mention|recall|note|add|talk|like to)\",'', sentence)\n",
    "    sentence = re.sub('May','',sentence)\n",
    "    \n",
    "    # remove capitalization, punctuations (dont remove numbers, dollar signs, full stops, commas)\n",
    "    sentence = re.sub(\"[^A-Za-z0-9$.,\\s]\",' ',sentence) \n",
    "    sentence = re.sub(' +',' ',sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.lower() \n",
    "    \n",
    "    # additional cleaning\n",
    "    sentence = re.sub(r\"\\b(?=[mdclxvii])m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})([ii]x|[ii]v|v?[ii]{0,3})\\b\\.?\", '', sentence)\n",
    "    sentence = re.sub(r'(mda|md a)','', sentence) # short form\n",
    "    sentence = re.sub(r'form\\s\\w{0,1}','',sentence) # form number\n",
    "    sentence = re.sub('table of contents','',sentence) # table of contents\n",
    "    sentence = re.sub(r'(item|i tem)\\s{0,1}[0-9]*[a-z]{0,1}','', sentence) # header\n",
    "    sentence = re.sub('(year|years) ended','', sentence)\n",
    "    sentence = re.sub('page\\s{0,1}[0-9]*','',sentence)\n",
    "    sentence = re.sub('rsquo','', sentence)\n",
    "    sentence = re.sub('amp','', sentence)\n",
    "    sentence = re.sub('rdquo','',sentence)\n",
    "    sentence = re.sub('ldquo','',sentence)\n",
    "    \n",
    "    # remove hanging characters\n",
    "#     sentence = re.sub(r'\\b[b-hj-z]\\b',' ', sentence) # remove hanging characters\n",
    "    sentence = re.sub(r'(?<!\\w)\\.(?!\\w)',' ',sentence) # remove hanging .\n",
    "    sentence = re.sub(' +',' ',sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module to get covfuture indicators\n",
    "# input: text as string\n",
    "# output: dictionary of relevant indicators\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ADD A NEW RULE TO THE PIPELINE\n",
    "subsentence_id = [\",\",\".\",\"!\",\"?\",\";\",\n",
    "                  \"or\",\"after\",\"because\",\"but\",\n",
    "                  \"so\", \"when\", \"where\", \"while\", \n",
    "                  \"although\", \"however\", \"though\", \"whereas\"\n",
    "                  \"so that\", \"despite\"]\n",
    "\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text in subsentence_id:\n",
    "            doc[token.i + 1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "\n",
    "def parse_text(text):\n",
    "    \"\"\"\n",
    "    Parses the input text and returns a dictionary of summed indicators (out1).\n",
    "    This includes:\n",
    "    - Tense flags (past/present/future)\n",
    "    - Covenant flags\n",
    "    - Expectation flags\n",
    "    - Word counts\n",
    "    - Summed stats for future-covenant sentences\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    sent_clean = clean_sentence(text)\n",
    "\n",
    "    # Parse with spaCy\n",
    "    doc_parse = nlp(sent_clean)\n",
    "    \n",
    "    # Prepare a container to collect sentence-level results\n",
    "    out = []\n",
    "\n",
    "    for i_sent, sent in enumerate(doc_parse.sents):\n",
    "        # 1) Count words in the current sentence (ignore punct/whitespace)\n",
    "        word_count = sum(1 for token in sent if not token.is_punct and not token.is_space)\n",
    "\n",
    "        # 2) Identify tense (past/present/future)\n",
    "        past_flag = 0\n",
    "        present_flag = 0\n",
    "        future_flag = 0\n",
    "        \n",
    "        # Past Tense\n",
    "        if (sent.root.tag_ in [\"VBD\", \"VBN\"]) or any(\n",
    "            (w.dep_ in [\"aux\", \"auxpass\"]) and (w.tag_ in [\"VBD\", \"VBN\"]) \n",
    "            for w in sent.root.children\n",
    "        ):\n",
    "            past_flag = 1\n",
    "\n",
    "        # Present Tense\n",
    "        present_tag = [\"VB\", \"VBG\", \"VBP\", \"VBZ\"]\n",
    "        nonpresent_tag = [\"VBD\", \"VBN\", \"MD\"]\n",
    "        if (sent.root.tag_ in present_tag) and not any(\n",
    "            (w.dep_ in [\"aux\", \"auxpass\"]) and (w.tag_ in nonpresent_tag)\n",
    "            for w in sent.root.children\n",
    "        ):\n",
    "            present_flag = 1\n",
    "\n",
    "        # Future Tense\n",
    "        if (sent.root.tag_ in present_tag) and any(\n",
    "            (w.dep_ in [\"aux\", \"auxpass\"]) and (w.tag_ == \"MD\") \n",
    "            for w in sent.root.children\n",
    "        ):\n",
    "            future_flag = 1\n",
    "\n",
    "        # 3) Find covenant keywords\n",
    "        cov_regex = r\"\\b(?:covenant|convenant)\"\n",
    "        covenant = re.findall(cov_regex, sent.text)\n",
    "        covenant_count = len(covenant)\n",
    "        covenant_flag = int(covenant_count > 0)\n",
    "\n",
    "        # 4) Find expectation keywords\n",
    "        expect_regex = r\"\\b(\" + \"|\".join(expect_keywords_all) + r\")\"\n",
    "        expect = re.findall(expect_regex, sent.text)\n",
    "        expect_count = len(expect)\n",
    "        expect_flag = int(expect_count > 0)\n",
    "\n",
    "        # 5) Determine future/past/active covenant mentions\n",
    "        query_cov_fut = 0\n",
    "        query_cov_act = 0\n",
    "        query_cov_past = 0\n",
    "        query_cov_fut_tense = 0\n",
    "\n",
    "        if (covenant_flag == 1) and (past_flag == 1):\n",
    "            if expect_flag == 0:\n",
    "                query_cov_past = 1\n",
    "        elif (covenant_flag == 1) and (present_flag == 1):\n",
    "            if expect_flag == 1:\n",
    "                query_cov_fut = 1\n",
    "            else:\n",
    "                query_cov_act = 1\n",
    "        elif (covenant_flag == 1) and (future_flag == 1):\n",
    "            query_cov_fut = 1\n",
    "            query_cov_fut_tense = 1\n",
    "        else:\n",
    "            if (covenant_flag == 1) and (expect_flag == 1):\n",
    "                query_cov_fut = 1\n",
    "\n",
    "        # 6) Collect results for this sentence in a dictionary\n",
    "        out.append({\n",
    "            \"query_covenant\": covenant_flag,\n",
    "            \"query_cov_past\": query_cov_past,\n",
    "            \"query_cov_act\": query_cov_act,\n",
    "            \"query_cov_fut\": query_cov_fut,\n",
    "            \"query_cov_fut_tense\": query_cov_fut_tense,\n",
    "            \"expect_keywords\": expect,  # list of matched expectation words\n",
    "            \"covmentions_fut\": [sent.text] if query_cov_fut else [],\n",
    "            \"covmentions_past\": [sent.text] if query_cov_past else [],\n",
    "            \"covmentions_act\": [sent.text] if query_cov_act else [],\n",
    "            # NEW: Word counts\n",
    "            \"word_count\": word_count,\n",
    "            \"expect_count\": expect_count,  # number of matched expectation words\n",
    "        })\n",
    "\n",
    "    # 7) Convert to DataFrame for easy summation\n",
    "    out_df = pd.DataFrame(out)\n",
    "\n",
    "    # 8) Summarize (same structure as your original code)\n",
    "    #    This sums up all numeric columns (including any new ones).\n",
    "    out1 = out_df.sum().to_dict()\n",
    "\n",
    "    # 9) Optionally, compute advanced sums for future-covenant sentences\n",
    "    #    (query_cov_fut == 1)\n",
    "    if \"query_cov_fut\" in out_df.columns:\n",
    "        fut_df = out_df[out_df[\"query_cov_fut\"] == 1]\n",
    "        out1[\"fut_sentences_word_count\"] = fut_df[\"word_count\"].sum() if not fut_df.empty else 0\n",
    "        out1[\"fut_sentences_expect_count\"] = fut_df[\"expect_count\"].sum() if not fut_df.empty else 0\n",
    "\n",
    "        # # 10) Add total words in the entire doc\n",
    "        # out1[\"total_words_in_document\"] = out_df[\"word_count\"].sum()\n",
    "    else:\n",
    "        out1[\"fut_sentences_word_count\"] = 0\n",
    "        out1[\"fut_sentences_expect_count\"] = 0\n",
    "        # out1[\"total_words_in_document\"] = 0\n",
    "\n",
    "    # Return the same final dictionary structure\n",
    "    return out1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main program starts here\n",
    "\n",
    "# read file\n",
    "s3_client = boto3.client('s3')\n",
    "file = f\"factset_calls_covmentions/covmentions_all.gzip\"\n",
    "obj = s3_client.get_object(Bucket=bucket,Key=file)\n",
    "df = pd.read_parquet(io.BytesIO(obj['Body'].read()))\n",
    "df.rename({'covmentions_raw':'covmentions_fullsent'},axis=1,inplace=True)\n",
    "\n",
    "# search for keywords\n",
    "print('finding forward-looking sentences...')\n",
    "query = df['covmentions_fullsent'].progress_apply(parse_text)\n",
    "query = pd.DataFrame(list(query))\n",
    "df1 = df.join(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a18ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save intermediate output\n",
    "\n",
    "output_prefix = f's3://{bucket}/output/'\n",
    "# save dataframe\n",
    "savepath = output_prefix + 'factset_calls_covenant_mentions_4_3.gzip'\n",
    "wr.s3.to_parquet(\n",
    "    df=df1,\n",
    "    path=savepath,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091cf69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample sentences\n",
    "test = df1[df1.query_cov_fut>0].sample(n=5).reset_index(drop=True)\n",
    "# test = df1[df1.query_cov_act>0].sample(n=5).reset_index(drop=True)\n",
    "for i in range(0,len(test)):\n",
    "    print(test.loc[i,'covmentions_fullsent'])\n",
    "    print(test.loc[i,'fut_sentences_word_count'])\n",
    "    print(test.loc[i, 'fut_sentences_expect_count'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output (note: word count is number of words in sentences with covmentions)\n",
    "df_out = df1[['date','repid','query_covenant','query_cov_fut','query_cov_act','query_cov_past','query_cov_fut_tense','fut_sentences_word_count','fut_sentences_expect_count','word_count']]  \n",
    "df_out = df_out.fillna(0)\n",
    "\n",
    "df_out.rename({'fut_sentences_word_count':'query_cov_fut_wc', 'fut_sentences_expect_count': 'query_expect_wc', 'word_count': 'query_covenant_wc'}, axis=1, inplace=True)\n",
    "    \n",
    "output_prefix = f's3://{bucket}/output/'\n",
    "savepath = output_prefix +'factset_calls_covenant_mentions_4_3.txt'\n",
    "wr.s3.to_csv(\n",
    "    df=df_out,\n",
    "    path=savepath,\n",
    "    sep='|'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
