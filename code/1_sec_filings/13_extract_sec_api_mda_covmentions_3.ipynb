{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify mentions of covenants in MDA section extracted from SEC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io \n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#setup\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# directories\n",
    "bucket = 'sagemaker-us-east-2-269018301143'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_clean(sentence):\n",
    "    \n",
    "    sentence = re.sub('\\n',' ', sentence) # remove line break markers \n",
    "    sentence = re.sub('&#[0-9]+;',' ', sentence) # remove character ids\n",
    "    sentence = re.sub(' +', ' ', sentence) # remove extra spaces\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences around covenant mentions / loan agreement amendments etc\n",
    "def parse_text(sentences, filing_date):\n",
    "    \n",
    "    # use dataframe structure\n",
    "    df = pd.DataFrame(columns = ['row','rawtext'])\n",
    "    df['rawtext'] = sentences\n",
    "    df['row'] = df.index\n",
    "    \n",
    "    # query covenant keywords\n",
    "    regex = r'\\b(?:covenant)'\n",
    "    df['query_cov'] = df['rawtext'].apply(lambda x: int(len(re.findall(regex,x))>0))\n",
    "\n",
    "    # get relevant text (up to 3 sentences following valid covenant mentions)\n",
    "    relevant_indices = df.index[df['query_cov'] == 1].tolist()\n",
    "    surrounding_indices = set()\n",
    "    \n",
    "    for index in relevant_indices:\n",
    "        start_index = max(0, index - 5)\n",
    "        end_index = min(len(df), index + 6)\n",
    "        surrounding_indices.update(range(start_index, end_index))\n",
    "    \n",
    "    surrounding_indices = sorted(surrounding_indices)\n",
    "    cov_full_text = ' '.join(df.loc[surrounding_indices, 'rawtext'].values)\n",
    "    \n",
    "    # save output\n",
    "    out = {'cov_full_text': cov_full_text}\n",
    "    \n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw text from input filepath\n",
    "\n",
    "def read_text(file, header=8):\n",
    "    \n",
    "    # get text\n",
    "    s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket=bucket,Key=file)\n",
    "    raw_text = obj['Body'].read().decode('utf-8').splitlines()\n",
    "        \n",
    "    # get file info\n",
    "    master_idx = re.search(r'(?<=<master idx>)(.*?)(?=</master idx>)', raw_text[0]).group(0)\n",
    "    cik = re.search(r'(?<=<cik>)(.*?)(?=</cik>)', raw_text[1]).group(0)\n",
    "    company_name = re.search(r'(?<=<company name>)(.*?)(?=</company name>)', raw_text[2]).group(0)\n",
    "    filing_type = re.search(r'(?<=<filing type>)(.*?)(?=</filing type>)', raw_text[3]).group(0)\n",
    "    filing_date = re.search(r'(?<=<filing date>)(.*?)(?=</filing date>)', raw_text[4]).group(0)\n",
    "    report_date = re.search(r'(?<=<report date>)(.*?)(?=</report date>)', raw_text[5]).group(0)\n",
    "    filing_index = re.search(r'(?<=<filing index>)(.*?)(?=</filing index>)', raw_text[6]).group(0)\n",
    "    filing_url = re.search(r'(?<=<filing url>)(.*?)(?=</filing url>)', raw_text[7]).group(0)\n",
    "    \n",
    "    # filter for incorrect text (old text mixed in)\n",
    "    valid_flag = 1\n",
    "    if len(raw_text)>=10:\n",
    "        incorrect_text = re.search(r'(^Our operating results may fluctuate significantly)',raw_text[9])\n",
    "        if incorrect_text:\n",
    "            valid_flag = 0\n",
    "    \n",
    "    # body of text\n",
    "    body = raw_text[header:] # remove header lines\n",
    "    if len(body)<5:\n",
    "        valid_flag = 0\n",
    "    \n",
    "    # join to single string\n",
    "    body = ' '.join(body) \n",
    "\n",
    "    # split into sentences\n",
    "    body = re.sub(r'(?<=No)\\.(?!\\w)', '',body) # dont tokenize \"No. 1\"\n",
    "    sentences = sent_tokenize(body)\n",
    "    sentences_clean = [simple_clean(s) for s in sentences] # clean and tokenize\n",
    "\n",
    "    # parse text\n",
    "    query = parse_text(sentences_clean, filing_date = filing_date)\n",
    "\n",
    "    return {\n",
    "        'master_idx': master_idx,\n",
    "        'cik': cik,\n",
    "        'company_name': company_name,\n",
    "        'filing_type': filing_type,\n",
    "        'filing_date': filing_date,\n",
    "        'report_date': report_date,\n",
    "        'filing_index': filing_index,\n",
    "        'valid_text': valid_flag,\n",
    "        'cov_full_text': query['cov_full_text'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read file paths\n",
    "\n",
    "yearstart = int(input('start year: '))\n",
    "yearend = int(input('end year: '))\n",
    "yrlist = range(yearstart,yearend+1)\n",
    "qtrlist = range(1,5)\n",
    "# qtrlist = range(1,2)\n",
    "\n",
    "for yr in reversed(yrlist):\n",
    "    \n",
    "    df_all = pd.DataFrame()\n",
    "    \n",
    "    for qtr in qtrlist:\n",
    "        \n",
    "        print(f'parsing text for year {yr} qtr {qtr} \\n')\n",
    "\n",
    "        prefix = f\"edgar_mda_new_2/{yr}/QTR{qtr}/\"\n",
    "        s3_client = boto3.client('s3')\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "        filepath = [obj['Key'] for page in pages for obj in page['Contents'] if '.txt' in obj['Key']]\n",
    "        \n",
    "        # get tokenized sentences and filing info (nested list: token -> sentence -> document)\n",
    "        df = Parallel(n_jobs=multiprocessing.cpu_count(), batch_size=32) \\\n",
    "                (delayed(read_text)(file, header=8) for file in tqdm(filepath)) \n",
    "        df = pd.DataFrame(df)\n",
    "        \n",
    "        df_all = df_all.append(df, ignore_index=True)\n",
    "        \n",
    "    output_prefix = f's3://{bucket}/edgar_mda_new_2_covmentions_june2024/'\n",
    "    savepath = output_prefix + f'{yr}_covmentions.gzip'\n",
    "    wr.s3.to_parquet(\n",
    "        df=df_all,\n",
    "        path=savepath,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine files and extract random sample for hand-labeling\n",
    "Note: this portion should be commented out when running the code above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bucket = 'sagemaker-us-east-2-269018301143'\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "yearlist = range(2000,2022)\n",
    "\n",
    "for yr in reversed(yearlist):\n",
    "        \n",
    "    print(f'reading file from year {yr}')\n",
    "\n",
    "    # read file\n",
    "    s3_client = boto3.client('s3')\n",
    "    file = f\"edgar_mda_new_2_covmentions_june2024/{yr}_covmentions.gzip\"\n",
    "    obj = s3_client.get_object(Bucket=bucket,Key=file)\n",
    "    df = pd.read_parquet(io.BytesIO(obj['Body'].read()))\n",
    "    \n",
    "    df_all = df_all.append(df, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with main master index list\n",
    "\n",
    "# import index file\n",
    "loadpath = f's3://{bucket}/misc/edgar_masterhtml1_combined.csv'\n",
    "indexdf = wr.s3.read_csv(path=loadpath,\n",
    "                        sep='|',\n",
    "                       lineterminator='\\n'\n",
    "                      )\n",
    "indexdf.rename({'Unnamed: 0':'master_idx'},axis=1,inplace=True)\n",
    "report_date = pd.to_datetime(indexdf.filing_date,format='%Y-%m-%d')\n",
    "indexdf['yq'] = pd.PeriodIndex(report_date, freq='Q').to_timestamp()\n",
    "indexdf.drop({'cik','company_name','filing_type','filing_date','report_date'},axis=1,inplace=True)\n",
    "\n",
    "# get yq \n",
    "df_all['parsed'] = 1\n",
    "df_all['master_idx'] = df_all['master_idx'].astype(int) \n",
    "\n",
    "# combine\n",
    "dfcombine = indexdf.merge(df_all, on='master_idx', how='left')\n",
    "dfcombine['parsed'] = dfcombine['parsed'].fillna(0)\n",
    "dfcombine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with covenant violation indicator\n",
    "\n",
    "file = f\"output/edgar_mda_new_violations_2_postsubmit.txt\"\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket,Key=file)\n",
    "cov_viol_ind = obj['Body'].read().decode('utf-8').splitlines()\n",
    "cov_viol_ind = '\\n'.join(cov_viol_ind)\n",
    "cov_viol_ind = pd.read_csv(io.StringIO(cov_viol_ind), sep='|')\n",
    "cov_viol_ind.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "dfcombine = dfcombine.merge(cov_viol_ind[['master_idx','cov_viol_ind']], on=['master_idx'], how='left',validate='1:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcombine.groupby(['yq'])['parsed'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full sample as gzip\n",
    "\n",
    "output_prefix = f's3://{bucket}/edgar_mda_new_2_covmentions_june2024/'\n",
    "savepath = output_prefix + f'covmentions_fullsample.gzip'\n",
    "wr.s3.to_parquet(\n",
    "    df=dfcombine,\n",
    "    path=savepath,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sample of 100 CIK indicators\n",
    "\n",
    "# extract sample of cik indicators (only those with at least 10 observations)\n",
    "cik_with_violation = dfcombine['cik'][dfcombine['cov_viol_ind']==1].unique()\n",
    "\n",
    "cik_list = dfcombine['cik'].value_counts()[dfcombine['cik'].value_counts()>=20].index\n",
    "cik_list = set(cik_list).intersection(set(cik_with_violation))\n",
    "\n",
    "random.seed(42)\n",
    "cik_sample = random.sample(list(cik_list), 20)\n",
    "\n",
    "# filter for sample\n",
    "df = dfcombine[dfcombine['cik'].isin(cik_sample)]\n",
    "df = df.sort_values(by=['cik','filing_date']).reset_index(drop=True)\n",
    "\n",
    "# save as excel\n",
    "output_prefix = f's3://{bucket}/edgar_mda_new_2_covmentions_june2024/'\n",
    "savepath = output_prefix + f'covmentions_sample100.xlsx'\n",
    "wr.s3.to_excel(\n",
    "    df=df,\n",
    "    path=savepath,\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
