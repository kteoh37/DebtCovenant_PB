{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts MD&A sections from SEC filings via the agency API and saves cleaned text for covenant analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sec-api\n",
    "\n",
    "from sec_api import ExtractorApi, FullTextSearchApi\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import boto3\n",
    "import json\n",
    "import awswrangler as wr\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "#setup\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# directories\n",
    "bucket = '[your-bucket-name]'  # Replace with your bucket name\n",
    "output_prefix = f's3://{bucket}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load api\n",
    "api_key = 'YOUR_SEC_API_KEY'  \n",
    "extractorApi = ExtractorApi(api_key)\n",
    "\n",
    "# Configure logging to save errors to a text file\n",
    "logging.basicConfig(\n",
    "    filename='errorlog_sec_api_extract_mda_2.txt',\n",
    "    level=logging.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract MDA using api\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=1)\n",
    "def get_mda(row_in):\n",
    "    \n",
    "    cik = row_in.cik\n",
    "    company_name = row_in.company_name\n",
    "    filing_type = row_in.filing_type\n",
    "    filing_date = row_in.filing_date\n",
    "    report_date = row_in.report_date\n",
    "    filing_index = row_in.filing_index\n",
    "    filing_url = row_in.main_url\n",
    "    \n",
    "    # for indexing files\n",
    "    master_idx = row_in.master_idx\n",
    "    yr = row_in.year\n",
    "    qtr = row_in.quarter\n",
    "    \n",
    "    # replace name of filingtype\n",
    "    filing_type = re.sub('[/.]','-',filing_type)\n",
    "    \n",
    "    # get text\n",
    "    try:\n",
    "        if 'K' in filing_type:\n",
    "            text = extractorApi.get_section(filing_url, \"7\", \"text\")\n",
    "        elif 'Q' in filing_type:\n",
    "            text = extractorApi.get_section(filing_url, \"part1item2\", \"text\")\n",
    "        else:\n",
    "            text = ''\n",
    "    except Exception as e:\n",
    "        \n",
    "        logging.error(f'Error occurred for CIK {cik} and URL {filing_url}: {str(e)}')\n",
    "        return None\n",
    "    \n",
    "    # save as text file\n",
    "    \n",
    "    textfile = ''\n",
    "    textfile += '<master idx>'+str(master_idx)+'</master idx> \\n'\n",
    "    textfile += '<cik>'+str(cik)+'</cik> \\n'\n",
    "    textfile += '<company name>'+str(company_name)+'</company name> \\n'\n",
    "    textfile += '<filing type>'+str(filing_type)+'</filing type> \\n'\n",
    "    textfile += '<filing date>'+str(filing_date)+'</filing date> \\n'\n",
    "    textfile += '<report date>'+str(report_date)+'</report date> \\n'\n",
    "    textfile += '<filing index>'+str(filing_index)+'</filing index> \\n'\n",
    "    textfile += '<filing url>'+str(filing_url)+'</filing url> \\n\\n'\n",
    "    \n",
    "    if len(text)>10:\n",
    "        textfile += text\n",
    "        savepath = 'edgar_mda_new_2/' + str(yr) + '/QTR' + str(qtr)  + '/' + str(master_idx) + '_' + str(report_date) + '_' + str(filing_date) + '_' + str(filing_type) + '_' + str(cik) +'.txt'\n",
    "        object = s3_resource.Object(bucket, savepath)\n",
    "        result = object.put(Body=textfile)\n",
    "        \n",
    "    else:\n",
    "        # error log\n",
    "        logging.error(f'Error occurred for CIK {cik} and URL {filing_url}: Text less than 10 characters.')\n",
    "        \n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index files\n",
    "loadpath = f's3://{bucket}/misc/edgar_masterhtml1_combined.csv'\n",
    "datdf = wr.s3.read_csv(path=loadpath,\n",
    "                        sep='|',\n",
    "                       lineterminator='\\n'\n",
    "                      )\n",
    "\n",
    "# generate relevant variables\n",
    "datdf.rename({'Unnamed: 0':'master_idx'},axis=1,inplace=True)\n",
    "datdf['filing_date1'] = pd.to_datetime(datdf['filing_date'])\n",
    "datdf['year'] = datdf['filing_date1'].dt.year.astype(int)\n",
    "datdf['quarter'] = datdf['filing_date1'].dt.quarter.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in datdf.iterrows():\n",
    "    \n",
    "    if (index) % 50 == 0:\n",
    "        print(f\"Processing idx {index+1} of {len(datdf)}: {row.year}-QTR{row.quarter}\")\n",
    "    \n",
    "    get_mda(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# read in parsed text and check how many files obtained\n",
    "# run this after compiling code above\n",
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_index(str_in):\n",
    "    \n",
    "    # Split the string by \"/\"\n",
    "    split_parts = str_in.split('/')\n",
    "\n",
    "    # Extract the desired substring\n",
    "    str_out = split_parts[-1].split('_')[0]\n",
    "    \n",
    "    return str_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yr = 2020\n",
    "qtr = 1\n",
    "\n",
    "index_list_all = pd.Series()\n",
    "\n",
    "yrlist = range(2000,2022)\n",
    "qtrlist = range(1,5)\n",
    "\n",
    "for yr in yrlist:\n",
    "    for qtr in qtrlist:\n",
    "        \n",
    "        print(f\"collecting file index for {yr}-QTR{qtr}\")\n",
    "\n",
    "        prefix = f\"edgar_mda_new_2/{yr}/QTR{qtr}/\"\n",
    "        s3_client = boto3.client('s3')\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "        filepath = [obj['Key'] for page in pages for obj in page['Contents'] if '.txt' in obj['Key']]\n",
    "\n",
    "        index_list = pd.Series(filepath).apply(get_file_index)\n",
    "        index_list_all = index_list_all.append(index_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with main index file\n",
    "\n",
    "index_df = pd.DataFrame(index_list_all)\n",
    "index_df.rename({0:'master_idx'}, axis=1, inplace=True)\n",
    "index_df['master_idx'] = index_df['master_idx'].astype(int) \n",
    "index_df['has_text'] = 1\n",
    "\n",
    "datdf1 = datdf.merge(index_df, on='master_idx', how='left')\n",
    "datdf1['has_text'] = datdf1['has_text'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many filings has text file\n",
    "\n",
    "subsample = datdf1[(datdf1.filing_type=='10-K')|(datdf1.filing_type=='10-Q')]\n",
    "\n",
    "print(f\"number of filings with text: {subsample.has_text.sum()}\")\n",
    "print(f\"total number of filings: {len(subsample)}\")\n",
    "print(f\"percentage: {subsample.has_text.sum()/len(subsample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chcek trends in filings over time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "yq = subsample['year'].astype(int).astype(str)+'-Q'+subsample['quarter'].astype(int).astype(str)\n",
    "subsample['yq'] = pd.PeriodIndex(yq, freq='Q').to_timestamp()\n",
    "plots = subsample.groupby('yq')['has_text'].mean()\n",
    "plt.plot(plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether filings exist\n",
    "\n",
    "sample0 = subsample[(subsample.has_text==0)].sample(n=1)\n",
    "url = sample0.main_url.values[0]\n",
    "filetype=sample0.filing_type.values[0]\n",
    "print(url)\n",
    "print(filetype)\n",
    "\n",
    "if 'K' in filetype:\n",
    "    print(extractorApi.get_section(url, \"7\", \"text\"))\n",
    "elif 'Q' in filetype:\n",
    "    print(extractorApi.get_section(url, \"part1item2\", \"text\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}