{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify covenant violations in MDA section extracted from SEC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "from nltk.stem import PorterStemmer\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#setup\n",
    "\n",
    "s3_resource = boto3.resource('s3')\n",
    "\n",
    "# directories\n",
    "bucket = '[your-bucket-name]'  # Replace with your bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate expect keywords\n",
    "\n",
    "def stem_words(wordlist):\n",
    "    stem_words = []\n",
    "    for x in wordlist:\n",
    "        xtoken = x.strip().split()\n",
    "        xstem = [ps.stem(t) for t in xtoken]\n",
    "        if len(xstem)>1:\n",
    "            out = ' '.join(xstem)\n",
    "            stem_words.append(out)\n",
    "        elif len(xstem)==1:\n",
    "            out = xstem[0]\n",
    "            stem_words.append(xstem[0])\n",
    "        else:\n",
    "            stem_words.append('')\n",
    "\n",
    "    return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action terms\n",
    "expect_keywords = [\n",
    "    'may becom','hope','outlook','go to','tailwind','work toward','trend','is like to','may depend','may not','forse','would','seek to','ought','potenti','could depend','unknown','remain confid','shortterm','depend','endeavor','intend','abl to remain','feel','may result','project','expect to','possibl','like will result','goal','may affect','go forward','belief','consid','estim will','contempl','suggest','pursu','call for','appear','well posit to','think','with a view to','appear to','up to','short term','prioriti','hypothes','can have','indic','may impact','schedul','envis','believ','could','look forward','pro forma','drive','uncertain','explor','could be','look forward to','see','prospect','upsid','may','should','is like','risk','improv','longterm','like','uncertainti','tent','forese','predict','would be','headwind','view','move toward','aim','estim','on target','pend','probabl','could potenti','might','may be','are like','pipelin','do not expect','may continu','seek','will','shall','not expect','will like result','futur','unanticip','guidanc','look ahead','likelihood','like to','full year guidanc','anticip','confid','opportun','propos','on pace','plan','schedul to','preliminari','will like','will like be','do not anticip','expect','presum','express confid','can be','opportunity','plans','believes','could potentially','is likely to','drive','predicting','may affect','may continue','uncertain','expect','headwind','would be','shall','depend','expressed confidence','projects','aims','looking forward','scheduled to','think','hopefully','on target','presume','seek to','view','looks forward','expects','belief','pending','may not','suggests','moving toward','depends','believe','goals','trend','do not expect','appear to'\n",
    "]\n",
    "\n",
    "violate_keywords = \\\n",
    "[\n",
    "    'waiv','viol','in default','modif','not in compliance','forbear',\n",
    "    'out of compliance','did not comply','unable to comply' 'failed to comply',\n",
    "    'did not meet', 'unable to meet', 'failed to meet', \n",
    "    'did not satisfy', 'unable to satisfy', 'failed to satisfy'\n",
    "]\n",
    "\n",
    "# misc \n",
    "exclude_keywords = \\\n",
    "[\n",
    "    'adjustable rate', 'gross margin', 'borrowing condition', 'floating rate',\n",
    "    'adjusted eurodollar rate', 'adjusted libor rate', 'exchange rate', 'adjustable margin',\n",
    "    'adjusted libor'\n",
    "]\n",
    "\n",
    "negate_keywords = \\\n",
    "[\n",
    "    'no', 'not', 'don', 'won', 'none', 'wouldn', 'without', 'didn'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic data cleaning. return tokens\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "\n",
    "    # remove formatting\n",
    "    sentence = re.sub('\\n',' ', sentence) # remove line break markers \n",
    "    sentence = re.sub('&#[0-9]+;',' ', sentence) # remove character ids\n",
    "    \n",
    "    # remove months etc\n",
    "    sentence = re.sub('covenant skills','', sentence)\n",
    "    sentence = re.sub('customer covenant','', sentence)\n",
    "    \n",
    "    # remove capitalization, punctuations (dont remove numbers, dollar signs, full stops, commas)\n",
    "    sentence = re.sub(\"[^A-Za-z0-9$.,\\s]\",' ',sentence) \n",
    "    sentence = re.sub(' +',' ',sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.lower() \n",
    "    \n",
    "    # additional cleaning\n",
    "    sentence = re.sub(r\"\\b(?=[mdclxvii])m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})([ii]x|[ii]v|v?[ii]{0,3})\\b\\.?\", '', sentence)\n",
    "    sentence = re.sub(r'(mda|md a)','', sentence) # short form\n",
    "    sentence = re.sub(r'form\\s\\w{0,1}','',sentence) # form number\n",
    "    sentence = re.sub('table of contents','',sentence) # table of contents\n",
    "    sentence = re.sub(r'(item|i tem)\\s{0,1}[0-9]*[a-z]{0,1}','', sentence) # header\n",
    "    sentence = re.sub('(year|years) ended','', sentence)\n",
    "    sentence = re.sub('page\\s{0,1}[0-9]*','',sentence)\n",
    "    sentence = re.sub('rsquo','', sentence)\n",
    "    sentence = re.sub('amp','', sentence)\n",
    "    sentence = re.sub('rdquo','',sentence)\n",
    "    sentence = re.sub('ldquo','',sentence)\n",
    "    \n",
    "    # remove hanging characters\n",
    "    sentence = re.sub(r'\\b[b-z]\\b',' ', sentence) # remove hanging characters\n",
    "    sentence = re.sub(r'(?<!\\w)\\.(?!\\w)',' ',sentence) # remove hanging .\n",
    "    sentence = re.sub(' +',' ',sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify dates from sentence and indicate whether it is one year before filing date\n",
    "\n",
    "def catch(func, handle=lambda e : e, *args, **kwargs):\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_olddate(sent,filing_date):\n",
    "    \n",
    "    any_olddate = 0\n",
    "\n",
    "    filing_date = datetime.strptime(filing_date,\"%Y-%m-%d\")\n",
    "    prevdate = filing_date-relativedelta(months=3) # filing date up to 2 months after end of quarter\n",
    "    \n",
    "    # date (e.g. jan 31, 2021)\n",
    "    regex = r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s(?:\\d{1,2},)\\s(?:19[7-9]\\d|2\\d{3})(?=\\D|$)'\n",
    "    datestr = re.findall(regex,sent)\n",
    "    datefmt0 = [catch(lambda : datetime.strptime(s, '%B %d, %Y')) for s in datestr]\n",
    "    \n",
    "    regex = re.compile('|'.join(datestr)) # remove pattern from sentence (so don't double parse)\n",
    "    sent = re.sub(regex, '',sent)\n",
    "    \n",
    "    # date format (e.g. jan 2021)\n",
    "    regex = r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december)\\s(?:19[7-9]\\d|2\\d{3})(?=\\D|$)'\n",
    "    datestr = re.findall(regex,sent)\n",
    "    datefmt1 = [catch(lambda : datetime.strptime(s, '%B %Y')) for s in datestr]\n",
    "    \n",
    "    regex = re.compile('|'.join(datestr)) # remove pattern from sentence (so don't double parse)\n",
    "    sent = re.sub(regex, '',sent)\n",
    "    \n",
    "    # date format (e.g. 2021) - assume refers to midyear month\n",
    "    regex = r'(?:(?<=\\D))(?:19[7-9]\\d|2[0-1]\\d{2})(?=\\D|$)'\n",
    "    datestr = re.findall(regex,sent)\n",
    "    datefmt2 = [catch(lambda : datetime.strptime(s, '%Y')+relativedelta(months=6)) for s in datestr]\n",
    "\n",
    "    # combine all parsed dates\n",
    "    datefmt = datefmt0 + datefmt1 + datefmt2\n",
    "    datefmt = [d for d in datefmt if d is not None]\n",
    "\n",
    "    olddate = 0\n",
    "    if len(datefmt) >0:\n",
    "        olddate = np.mean([1 if (d is not None) and (d < prevdate) else 0 for d in datefmt])\n",
    "    \n",
    "    any_olddate = 0\n",
    "    if olddate >= 0.5:\n",
    "        any_olddate = 1\n",
    "    \n",
    "    return any_olddate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences around covenant mentions / loan agreement amendments etc\n",
    "def parse_text(sentences, filing_date):\n",
    "    \n",
    "    # use dataframe structure\n",
    "    df = pd.DataFrame(columns = ['row','rawtext'])\n",
    "    df['rawtext'] = sentences\n",
    "    df['row'] = df.index\n",
    "\n",
    "    # delete negative or hypothetical action terms\n",
    "    regex = r'\\b(?:'+'|'.join(negate_keywords)+r')[A-z0-9\\s]{0,10}\\b(?:' + '|'.join(violate_keywords) + r')'\n",
    "    df['text'] = df['rawtext'].apply(lambda x: re.sub(regex, '', x))\n",
    "\n",
    "    # query covenant keywords\n",
    "    regex = r'\\b(?:covenant)'\n",
    "    df['query_cov'] = df['text'].apply(lambda x: int(len(re.findall(regex,x))>0))\n",
    "    \n",
    "    # query violation keywords\n",
    "    regex = r'\\b(?:' + '|'.join(violate_keywords) + r')'\n",
    "    df['word_viol'] = df['text'].apply(lambda x: re.findall(regex,x))\n",
    "    df['query_viol'] = df['word_viol'].apply(lambda x: int(len(x)>0))\n",
    "    \n",
    "    # query expect keywords\n",
    "    regex = r'\\b(?:' + '|'.join(expect_keywords) + r')'\n",
    "    df['word_expect'] = df['text'].apply(lambda x: re.findall(regex,x))\n",
    "    df['query_expect'] = df['word_expect'].apply(lambda x: int(len(x)>0))\n",
    "    \n",
    "    # search for year in amendment sentence\n",
    "    df['query_olddate'] = df['text'].apply(lambda x: find_olddate(x,filing_date))\n",
    "\n",
    "    # check if mentions corresponds to correct date range\n",
    "    mask = (df.query_olddate>0)|(df.query_expect>0)\n",
    "    df.loc[mask,'query_cov'] = 0\n",
    "    df.loc[mask,'query_viol'] = 0\n",
    "\n",
    "    # get relevant text (up to 3 sentences following valid covenant mentions)\n",
    "    df['l1'] = df.query_cov.shift(1)\n",
    "    df['l2'] = df.query_cov.shift(2)\n",
    "    df['l3'] = df.query_cov.shift(3)\n",
    "    df['_all'] = df[['query_cov','l1','l2','l3']].sum(axis=1)    \n",
    "\n",
    "    # save output\n",
    "    out = {}\n",
    "    cov_full_text = None\n",
    "    cov_viol_text = None\n",
    "    cov_viol_keywords = None\n",
    "    cov_viol_ind = 0\n",
    "\n",
    "    if df._all.sum()>0:\n",
    "\n",
    "        df = df[df._all>0]\n",
    "        cov_full_text = ' '.join(list(df['text'].values))\n",
    "        \n",
    "        if df.query_viol.sum()>0:\n",
    "            cov_viol_text = df.loc[df.query_viol>0,'text'].sum()\n",
    "            cov_viol_keywords = df.loc[df.query_viol>0,'word_viol'].sum()\n",
    "            cov_viol_ind = 1\n",
    "        \n",
    "       \n",
    "    # save output\n",
    "    out['cov_full_text'] = cov_full_text\n",
    "    out['cov_viol_text'] = cov_viol_text\n",
    "    out['cov_viol_keywords'] = cov_viol_keywords\n",
    "    out['cov_viol_ind'] = cov_viol_ind\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw text from input filepath\n",
    "\n",
    "def read_text(file, header=7):\n",
    "    \n",
    "    # get text\n",
    "    s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket=bucket,Key=file)\n",
    "    raw_text = obj['Body'].read().decode('utf-8').splitlines()\n",
    "        \n",
    "    # get file info\n",
    "    master_idx = re.search(r'(?<=<master idx>)(.*?)(?=</master idx>)', raw_text[0]).group(0)\n",
    "    cik = re.search(r'(?<=<cik>)(.*?)(?=</cik>)', raw_text[1]).group(0)\n",
    "    company_name = re.search(r'(?<=<company name>)(.*?)(?=</company name>)', raw_text[2]).group(0)\n",
    "    filing_type = re.search(r'(?<=<filing type>)(.*?)(?=</filing type>)', raw_text[3]).group(0)\n",
    "    filing_date = re.search(r'(?<=<filing date>)(.*?)(?=</filing date>)', raw_text[4]).group(0)\n",
    "    report_date = re.search(r'(?<=<report date>)(.*?)(?=</report date>)', raw_text[5]).group(0)\n",
    "    filing_index = re.search(r'(?<=<filing index>)(.*?)(?=</filing index>)', raw_text[6]).group(0)\n",
    "    filing_url = re.search(r'(?<=<filing url>)(.*?)(?=</filing url>)', raw_text[7]).group(0)\n",
    "    \n",
    "    # filter for incorrect text (old text mixed in)\n",
    "    valid_flag = 1\n",
    "    if len(raw_text)>=10:\n",
    "        incorrect_text = re.search(r'(^Our operating results may fluctuate significantly)',raw_text[9])\n",
    "        if incorrect_text:\n",
    "            valid_flag = 0\n",
    "    \n",
    "    # body of text\n",
    "    body = raw_text[header:] # remove header lines\n",
    "    if len(body)<5:\n",
    "        valid_flag = 0\n",
    "    \n",
    "    # join to single string\n",
    "    body = ' '.join(body) \n",
    "\n",
    "    # split into sentences\n",
    "    body = re.sub(r'(?<=No)\\.(?!\\w)', '',body) # dont tokenize \"No. 1\"\n",
    "    sentences = sent_tokenize(body)\n",
    "    \n",
    "    # clean sentence\n",
    "    sentences_clean = [clean_sentence(s) for s in sentences] # clean and tokenize\n",
    "#     sentences_clean = [s for s in sentences_clean if len(s) > 1] # remove sentences that are empty strings\n",
    "    \n",
    "    # parse text\n",
    "    query = parse_text(sentences_clean, filing_date = filing_date)\n",
    "\n",
    "    return {\n",
    "        'master_idx': master_idx,\n",
    "        'cik': cik,\n",
    "        'company_name': company_name,\n",
    "        'filing_type': filing_type,\n",
    "        'filing_date': filing_date,\n",
    "        'report_date': report_date,\n",
    "        'filing_index': filing_index,\n",
    "        'valid_text': valid_flag,\n",
    "        'cov_full_text': query['cov_full_text'],\n",
    "        'cov_viol_text': query['cov_viol_text'],\n",
    "        'cov_viol_keywords': query['cov_viol_keywords'],\n",
    "        'cov_viol_ind': query['cov_viol_ind']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read file paths\n",
    "\n",
    "yearstart = int(input('start year: '))\n",
    "yearend = int(input('end year: '))\n",
    "yrlist = range(yearstart,yearend+1)\n",
    "qtrlist = range(1,5)\n",
    "# qtrlist = range(1,2)\n",
    "\n",
    "for yr in reversed(yrlist):\n",
    "    \n",
    "    df_all = pd.DataFrame()\n",
    "    \n",
    "    for qtr in qtrlist:\n",
    "        \n",
    "        print(f'parsing text for year {yr} qtr {qtr} \\n')\n",
    "\n",
    "        prefix = f\"edgar_mda_new_2/{yr}/QTR{qtr}/\"\n",
    "        s3_client = boto3.client('s3')\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "        filepath = [obj['Key'] for page in pages for obj in page['Contents'] if '.txt' in obj['Key']]\n",
    "        \n",
    "        # get tokenized sentences and filing info (nested list: token -> sentence -> document)\n",
    "        df = Parallel(n_jobs=multiprocessing.cpu_count(), batch_size=32) \\\n",
    "                (delayed(read_text)(file, header=7) for file in tqdm(filepath)) \n",
    "        df = pd.DataFrame(df)\n",
    "        \n",
    "        df_all = df_all.append(df, ignore_index=True)\n",
    "        \n",
    "    output_prefix = f's3://{bucket}/edgar_mda_new_2_violations/'\n",
    "    savepath = output_prefix + f'{yr}_violations.gzip'\n",
    "    wr.s3.to_parquet(\n",
    "        df=df_all,\n",
    "        path=savepath,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract numerical indicators as stata input\n",
    "Note: the code above is run in python script (takes about 4 hours to run)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "\n",
    "# yearstart = int(input('start year: '))\n",
    "# yearend = int(input('end year: '))\n",
    "# yearlist = range(yearstart,yearend+1)\n",
    "yearlist = range(2000,2022)\n",
    "\n",
    "for yr in reversed(yearlist):\n",
    "        \n",
    "    print(f'reading file from year {yr} \\n')\n",
    "\n",
    "    # read file\n",
    "    s3_client = boto3.client('s3')\n",
    "    file = f\"edgar_mda_new_2_violations/{yr}_violations.gzip\"\n",
    "    obj = s3_client.get_object(Bucket=bucket,Key=file)\n",
    "    df = pd.read_parquet(io.BytesIO(obj['Body'].read()))\n",
    " \n",
    "    # extract indicators\n",
    "    df = df[df.valid_text==1].reset_index(drop=True)\n",
    "    df_out = df[['master_idx','cik','company_name','filing_type','filing_date','report_date',\n",
    "                 'cov_viol_ind']]  \n",
    "    df_out = df_out.fillna(0)\n",
    "    \n",
    "    df_all = df_all.append(df_out, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output to file\n",
    "output_prefix = f's3://{bucket}/output/'\n",
    "savepath = output_prefix +'edgar_mda_new_violations_2_postsubmit.txt'\n",
    "wr.s3.to_csv(\n",
    "    df=df_all,\n",
    "    path=savepath,\n",
    "    sep='|'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many successfully parsed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import index file\n",
    "loadpath = f's3://{bucket}/misc/edgar_masterhtml1_combined.csv'\n",
    "indexdf = wr.s3.read_csv(path=loadpath,\n",
    "                        sep='|',\n",
    "                       lineterminator='\\n'\n",
    "                      )\n",
    "indexdf.rename({'Unnamed: 0':'master_idx'},axis=1,inplace=True)\n",
    "report_date = pd.to_datetime(indexdf.filing_date,format='%Y-%m-%d')\n",
    "indexdf['yq'] = pd.PeriodIndex(report_date, freq='Q').to_timestamp()\n",
    "indexdf.drop({'cik','company_name','filing_type','filing_date','report_date'},axis=1,inplace=True)\n",
    "\n",
    "# get yq \n",
    "df_all['parsed'] = 1\n",
    "df_all['master_idx'] = df_all['master_idx'].astype(int) \n",
    "\n",
    "# combine\n",
    "dfcombine = indexdf.merge(df_all, on='master_idx', how='left')\n",
    "dfcombine['parsed'] = dfcombine['parsed'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(2)\n",
    "\n",
    "plots = dfcombine.groupby('yq')['parsed'].mean()\n",
    "axs[0].plot(plots)\n",
    "\n",
    "subsample = dfcombine[(dfcombine.filing_type=='10-K')|(dfcombine.filing_type=='10-Q')]\n",
    "plots = subsample.groupby('yq')['cov_viol_ind'].mean()\n",
    "axs[1].plot(plots)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
