{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract header section containing safe harbor statements from filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import collections\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for covenant violation mentions, following algo by Nini Smith Sufi (2012)\n",
    "def get_cautionary_text(raw_text):\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['row','raw_text'])\n",
    "    df['raw_text'] = [re.sub(' +',' ',r) for r in raw_text]\n",
    "    df['row'] = df.index\n",
    "\n",
    "    # get relevant lines \n",
    "    regex = re.compile(r'^(?:.{0,50})(?:f\\s*o\\s*r\\s*w\\s*a\\s*r\\s*d\\s*-{0,1}\\s*l\\s*o\\s*o\\s*k\\s*i\\s*n\\s*g)', re.IGNORECASE)\n",
    "    start_row = df.loc[(df['raw_text'].str.contains(regex)),'row']\n",
    "    \n",
    "    out_list = []\n",
    "    out_sent = []\n",
    "    \n",
    "    if len(start_row)>0:\n",
    "        start_row = start_row.reset_index().loc[0,'row']\n",
    "        out_text = ' '.join(df.loc[start_row:start_row+3,'raw_text'])   \n",
    "        \n",
    "        # get words in doubel quotation marks\n",
    "        find_str = re.findall(r'(?<=words)(.*?)(?=\\b(?:or|and|the negative of|not limited to|the negatives)\\b)', out_text)\n",
    "        if len(find_str)>0:\n",
    "            word_list = find_str[0] # get first match\n",
    "            word_list = word_list.split(',')\n",
    "            word_list = [re.sub('[^A-z\\s]','',x) for x in word_list if len(x)<25]\n",
    "            \n",
    "            if any('expect' in word for word in word_list):\n",
    "                out_list = [word for word in word_list if not any(e in word for e in ['such','as']) and len(word)>1]\n",
    "                out_sent = find_str[0]\n",
    "                \n",
    "    return out_list, out_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper to read files in parallel\n",
    "def parse_wrapper(file):\n",
    "\n",
    "    # read file\n",
    "    with open(file) as f:\n",
    "        raw_text = f.readlines()\n",
    "\n",
    "    # get header info\n",
    "    header_idx = file.rfind('/') # last occurrence of forward slash\n",
    "    filename = file[header_idx+1:]\n",
    "    pos_idx = [x.start() for x in re.finditer(r'_', filename)]\n",
    "\n",
    "    filing_date = filename[:pos_idx[0]]\n",
    "    filing_type = filename[pos_idx[0]+1:pos_idx[1]]\n",
    "    cik = filename[pos_idx[3]+1:pos_idx[4]]\n",
    "    \n",
    "    # only parse text of 10K or 10q (or related variants)\n",
    "    word_list = []\n",
    "    sentence = []\n",
    "    if any(s in filing_type for s in ['10-K','10-Q']):\n",
    "        try:\n",
    "            # get mda section\n",
    "            word_list, sentence = get_cautionary_text(raw_text)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return {  \n",
    "          'cautionary_words': word_list,\n",
    "          'sentence_example': sentence\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load file from folder\n",
    "\n",
    "yrlist = range(2000, 2022)\n",
    "# yrlist= [2004]\n",
    "qtrlist = range(1, 5)\n",
    "# qtrlist = [1]\n",
    "\n",
    "all_words = list()\n",
    "all_sentences = list()\n",
    "\n",
    "for yr in reversed(yrlist):\n",
    "    \n",
    "    out_df_yr = pd.DataFrame()\n",
    "    for qtr in qtrlist:\n",
    "        \n",
    "        print('year: '+ str(yr) + ' quarter: '+ str(qtr))\n",
    "        \n",
    "        # read in folder\n",
    "        folder = '../../rawdata/edgar_mcdonald/' + str(yr) + '/QTR' + str(qtr) + '/*.txt'\n",
    "        filepath = glob.glob(folder)\n",
    "        \n",
    "        out_df = Parallel(n_jobs=multiprocessing.cpu_count(), batch_size=32) \\\n",
    "                (delayed(parse_wrapper)(file) for file in tqdm(filepath))  \n",
    "        out_df = pd.DataFrame(out_df)\n",
    "\n",
    "        cautionary_words = [y for x in out_df.cautionary_words.values for y in x if x]\n",
    "        all_words.extend(cautionary_words)\n",
    "        \n",
    "        sentences = [sent for sent in out_df.loc[out_df.sentence_example.astype(bool),'sentence_example']]\n",
    "        all_sentences.extend(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most frequent vocab \n",
    "\n",
    "print(len(all_words))\n",
    "\n",
    "# raw version\n",
    "all_words = [x.strip() for x in all_words]\n",
    "frequency = collections.Counter(all_words)\n",
    "most_common = pd.DataFrame(frequency.most_common(1500), columns=['word','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if word comes from the correct section\n",
    "\n",
    "for s in all_sentences:\n",
    "    if 'world' in s:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates in cleaned list and then stem\n",
    "# note that raw list is filtered for repeated keywords\n",
    "\n",
    "# read in list \n",
    "with open('../../temp/safe_harbor_most_common_keywords_raw.txt') as f:\n",
    "    keywords = f.read().splitlines()\n",
    "\n",
    "# filter for duplicates\n",
    "keywords = list(set(keywords))\n",
    "\n",
    "with open(\"../../temp/safe_harbor_most_common_keywords_clean.txt\", 'w') as output:\n",
    "    for word in keywords:\n",
    "        output.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem words\n",
    "stem_words = []\n",
    "for x in tqdm(keywords):\n",
    "    xtoken = x.strip().split()\n",
    "    xstem = [ps.stem(t) for t in xtoken]\n",
    "    if len(xstem)>1:\n",
    "        out = ' '.join(xstem)\n",
    "        stem_words.append(out)\n",
    "    elif len(xstem)==1:\n",
    "        out = xstem[0]\n",
    "        stem_words.append(xstem[0])\n",
    "    else:\n",
    "        next\n",
    "\n",
    "stem_words = list(set(stem_words))\n",
    "        \n",
    "with open(\"../../temp/safe_harbor_most_common_keywords_clean_stem.txt\", 'w') as output:\n",
    "    for word in stem_words:\n",
    "        if word!=stem_words[-1]:\n",
    "            output.write(word + '\\n')\n",
    "        else:\n",
    "            output.write(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(most_common))\n",
    "print(len(keywords))\n",
    "print(len(stem_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
